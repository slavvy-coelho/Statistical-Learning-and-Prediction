---
title: "Assignment 3"
author: "Savitaa Venkateswaran, Ruchita Robert Rozario, Slavvy Coelho"
date: '2019-10-04'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions

Fill in your computations and answers to the assignment questions in this
RMarkdown document. When you are finished, click the "Knit" button on RStudio
to render an HTML document. You can then use your browser or tool of choice
to convert the HTML document to a PDF file.

This assignment is to be handed in through canvas on Monday Oct 7 at 11:00pm.
(Note that this due date is different from the due date given on the canvas
Admin page.) This is a group assignment.
You must join a group on canvas even if you want to work alone. Please upload one PDF file
with your solutions per group. 

## Question 1 (Chapter 3, #15)
15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.

(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.

#### Predictor- age vs crim 
```{r}
library(MASS)
attach(Boston)
fit_age = lm(crim ~ age)
summary(fit_age)
```

#### Predictor- black vs crim
```{r}
fit_black = lm(crim ~ black)
summary(fit_black)
```

#### Predictor- chas vs crim
```{r}
chas = as.factor(chas)
fit_chas = lm(crim ~ chas)
summary(fit_chas)
```

#### Predictor- dis vs crim
```{r}
fit_dis = lm(crim ~ dis)
summary(fit_dis)
```

#### Predictor- indus vs crim
```{r}
fit_indus = lm(crim ~ indus)
summary(fit_indus)
```

#### Predictor- lstat vs crim
```{r}
fit_lstat = lm(crim ~ lstat)
summary(fit_lstat)
```

#### Predictor- medv vs crim
```{r}
fit_medv = lm(crim ~ medv)
summary(fit_medv)
```

#### Predictor- nox vs crim
```{r}
fit_nox = lm(crim ~ nox)
summary(fit_nox)
```

#### Predictor- ptratio vs crim
```{r}
fit_ptratio = lm(crim ~ ptratio)
summary(fit_ptratio)
```

#### Predictor- rad vs crim
```{r}
rad = as.factor(rad)
fit_rad = lm(crim ~ rad)
summary(fit_rad)
```

#### Predictor- rm vs crim
```{r}
fit_rm = lm(crim ~ rm)
summary(fit_rm)
```

#### Predictor- tax vs crim
```{r}
fit_tax = lm(crim ~ tax)
summary(fit_tax)
```

#### Predictor- zn vs crim
```{r}
fit_zn = lm(crim ~ zn)
summary(fit_zn)
```

#### Observation:
Predictors having significant impact can be deduced using the test H0:β1=0. All predictors from the boston dataset have a "p-value" less than 0.05 except “chas”. This concludes that there is a statistically significant relationship between each predictor variables and the response (crim) variable except for the “chas” and "age" being a relatively weakly associated predictor variable (when compared to others).


#### To back up our observation:
##### plot linear fit between chas vs crim lm fit plot
```{r}
library(ggplot2)
ggplot(Boston, aes(x = chas, y = crim)) + 
    geom_point() +
    stat_smooth(method = "lm", col = "red")
```

##### plot() function to visually examine the relationship between chas and crim variables
```{r}
plot(fit_chas)
```

##### plot linear fit between age vs crim lm fit plot
```{r}
ggplot(Boston, aes(x = age, y = crim)) + 
    geom_point() +
    stat_smooth(method = "lm", col = "red")
```

##### plot() function to visually examine the relationship between age and crim variables
```{r}
plot(fit_age)
```


(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H0 : βj = 0?

```{r}
fit_all <- lm(crim ~ ., data = Boston)
summary(fit_all)
```

#### Summary of the results obtained:
For the following predictors: “zn”, “dis”, “rad”, “black” and “medv” of the Boston dataset, Null Hypothesis can be rejected as their p-values are lesser than 0.05 and hence, making them statiscally significant in predicting the repsonse variable (crim) in boston dataset.

(c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.

```{r}
single_reg <- vector("numeric",0)
single_reg <- c(single_reg, fit_zn$coefficient[2])
single_reg <- c(single_reg, fit_indus$coefficient[2])
single_reg <- c(single_reg, fit_chas$coefficient[2])
single_reg <- c(single_reg, fit_nox$coefficient[2])
single_reg <- c(single_reg, fit_rm$coefficient[2])
single_reg <- c(single_reg, fit_age$coefficient[2])
single_reg <- c(single_reg, fit_dis$coefficient[2])
single_reg <- c(single_reg, fit_rad$coefficient[2])
single_reg <- c(single_reg, fit_tax$coefficient[2])
single_reg <- c(single_reg, fit_ptratio$coefficient[2])
single_reg <- c(single_reg, fit_black$coefficient[2])
single_reg <- c(single_reg, fit_lstat$coefficient[2])
single_reg <- c(single_reg, fit_medv$coefficient[2])
multiple_reg <- vector("numeric", 0)
multiple_reg <- c(multiple_reg, fit_all$coefficients)
multiple_reg<- multiple_reg[-1]
plot(single_reg, multiple_reg, col = "red")
```

```{r}
cor(Boston[-c(1, 4)])
```

#### Inference: 
The results obtained from simple linear regression might be offset from the results obtained from multiple regression. This is due to the fact that we consider the rate of change of a single predictor variable affecting the response variable. However, incase of multiple linear regression, inorder to understand the relationship between a predictor and the corresponding response variable we have to keep the other features/predictor variables fixed. This affects the relationship strength. This makes sense for multiple regression to suggest no relationship between the response and some of the predictors while the simple linear regression implies the opposite because the correlation between the predictors show some strong relationships between some of the predictors. This can be clearly observed from the corrleation obtained above, especially with regard to the the 'age' variable.


(d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form
Y = β0 +β1X +β2X2 +β3X3 +ε.

#### Polynomial Regression model for each predictor vs crim
##### Predictor- age vs crim 
```{r}
fit_age = lm(crim ~ poly(age, 3))
summary(fit_age)
```

##### Predictor- black vs crim
```{r}
fit_black = lm(crim ~ poly(black, 3))
summary(fit_black)
```

##### Predictor- dis vs crim
```{r}
fit_dis = lm(crim ~ poly(dis, 3))
summary(fit_dis)
```

##### Predictor- indus vs crim
```{r}
fit_indus = lm(crim ~ poly(indus, 3))
summary(fit_indus)
```

##### Predictor- lstat vs crim
```{r}
fit_lstat = lm(crim ~ poly(lstat, 3))
summary(fit_lstat)
```

##### Predictor- medv vs crim
```{r}
fit_medv = lm(crim ~ poly(medv, 3))
summary(fit_medv)
```

##### Predictor- nox vs crim
```{r}
fit_nox = lm(crim ~ poly(nox, 3))
summary(fit_nox)
```

##### Predictor- ptratio vs crim
```{r}
fit_ptratio = lm(crim ~ poly(ptratio, 3))
summary(fit_ptratio)
```

##### Predictor- rm vs crim
```{r}
fit_rm = lm(crim ~ poly(rm, 3))
summary(fit_rm)
```

##### Predictor- tax vs crim
```{r}
fit_tax = lm(crim ~ poly(tax, 3))
summary(fit_tax)
```

##### Predictor- zn vs crim
```{r}
fit_zn = lm(crim ~ poly(zn, 3))
summary(fit_zn)
```

#### Inference:
The variables age,dis,indus,nox,medv,ptratio show statical significance using higher degree regressors (using polynomial regression), the p values are lesser than 0.05. This concludes they have some non-linear relationship with the response variable. However, other than the above mentioned features- none of the others depict any significant relationship; p-values are greater than or equal to 0.05. This concludes that in the latter case there are no non-linear trends between the different predictors and then response variable (crim).

## Question 2 (Chapter 4, #4)

(a) p=1
X is uniformly distributed on [0,1].
Predict:  fraction within 10% range of test observations.
Solution:
Since it is a uniform distribution,
For X=0.6,		range = [0.55, 0.65]
Therefore, for any X,
(0.65-0.55)/(1-0) = 0.10 	
ie. 10% of the total observation.
Since X is evenly distributed, if there is 10% of the total lying in the given range, any range will have the same number of observations as that range.

(b) p=2 (two features X1 and X2 are uniformly distributed.)
Predict: fraction within 10% range of test observations.
Solution:
Since it is a uniform distribution,
For X=0.6,		range = [0.55, 0.65]
For X=0.35		range = [0.3,0.4]
Since X1 and X2 are independent variables, 
(10% * 10%) = (10%)2 = 0.01 = 1%

(c) p=100
From the above cases, we can generalize that
For a uniform distribution,
Fractions to be used = (10%)p
When p=100, 
Fraction = (10%)p => almost negligible

(d) From a, b and c
We observe that as we increase the number of features p, the percentage of observations that can be used to predict with KNN becomes very small.

(e) when 
	P		   side
	1		   0.1
	2		  (0.1)1/2=0.316
	100		(0.1)1/100=0.977
Here as p increases, we need to include almost entire range of the considered features.



## Question 3 (Chapter 4, #10 parts (a)-(h), 9 marks)

```{r}
library(ISLR)
data(Weekly)
head(Weekly)
```


(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns
```{r}
summary(Weekly)
pairs(Weekly)
cor(Weekly[, -9])
attach(Weekly) 
plot(Volume) #Volume increased with time
```
From the pairs(Weekly) plot it was clear that only Volume showed a proper trend with respect to the year attribute. (The volume increases with year)

(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?
```{r}
glm = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly,
              family = binomial)
summary(glm)
```
Lag2 has its p-value is less than 0.05 and thus is statistically significant.

(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
```{r}
glm_init = predict(glm, type = "response")
glm_predict = rep("Down", length(glm_init))
glm_predict[glm_init > 0.5] = "Up"
table(glm_predict, Direction) 

```
Percentage of correct predictions on the training data is given by the diagonal elements. (54+557)/1089 =56.1065197%.
For weeks when the market goes up, the model is right 92.06% of the time (557/(48+557)). 
For weeks when the market goes down, the model is right only 11.15% of the time (54/(54+430)).

(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).
```{r}
train <- (Year < 2009)
WeeklyTrim <- Weekly[!train, ]
DirectionTrim <- Direction[!train]
fglm2 <- glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
summary(fglm2)

mod2 <- predict(fglm2, WeeklyTrim, type = "response")
predict_glm2 <- rep("Down", length(mod2))
predict_glm2[mod2 > 0.5] <- "Up"
table(predict_glm2, DirectionTrim)
```
Percentage of correct predictions on the test data is (9+56)/104= 62.5%
For weeks when the market goes up, the model is right 91.80% of the time (56/(56+5)). 
For weeks when the market goes down, the model is right only 20.93% of the time (9/(9+34)).

(e) LDA
```{r}
library(MASS)
lda <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda
predict_lda <- predict(lda, WeeklyTrim)
table(predict_lda$class, DirectionTrim)
```
Percentage of correct predictions on the test data is 62.5%.
For weeks when the market goes up, the model is right 91.80%  of the time.
For weeks when the market goes down, the model is right only 20.93%of the time.

(f) QDA
```{r}
qda <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda
predict_qda <- predict(qda, WeeklyTrim)
table(predict_qda$class, DirectionTrim)
```
Percentage of correct predictions on the test data is 58.65%.
For weeks when the market goes up, the model is right 100%  of the time.
For weeks when the market goes down, the model is right 0% of the time.

(g)  KNN
```{r}
library(class)
trainX <- as.matrix(Lag2[train])
testX <- as.matrix(Lag2[!train])
trainDir <- Direction[train]
set.seed(1)
predict_knn <- knn(trainX, testX, trainDir, k = 1) 
table(predict_knn, DirectionTrim)
```
Percentage of correct predictions on the test data is 50%.
For weeks when the market goes up, the model is right 50.82%  of the time.
For weeks when the market goes down, the model is right only 48.83% of the time.

(h)  Comparison

Decreasing order of error rate:
1. Logistic Regression
2. LDA
3. QDA
4. KNN
Thus, in our scenario, Logistic regression and LDA performed equally well followed by QDA and KNN.